---
title: "1, 3 November"
output: html_document
date: "2023-11-01"
---

## 1 November - Wrangling Multinomial Data 
  - multinomial = more than 2 outomes (2 is bernoulli)

GGG Kaggle Competition - submissions are evaluated on the categorization accuracy 
  - identifiers are explanatory variables 
  - mosaic plot: shows the relationship between categorical response and categorical explanatory 
  
  
# Adjustments for Multinomial Outcomes 
  - for bernoulli we use AUC/ROC: y is correct, x is missed (for every prediction i've missed, how many have I gotten right) 
      - also, precision: of those that i predict 'yes', which actually are yes 
      - recall: of those that were actually yes, how much did i predict as a yes
      - specificity: of those that are actually no, what % did i get correct 
      - sensitivity (or accuracy): did I correctly predict the yes and nos 
      
  - multinomial outcomes are relative to bernoulli, but adjustments are required 
      - one vs. all: ghost (yes) or something else (no), then goblin (yes) .. etc. then average (default in tidymodels) 
      - one vs. one: consider all combinations of categories (n choose 2) - then average across all considerations 
      
      
# Data Imputation 
what do we do if we have missing values in our explanatory variables?  
  - throw them out (step_naomit()) 
     - you could lose a lotttt of data; rids the whole line 
     - sooo easy tho.. 
  - knn! - predict them using everything but the response 
what to do if there are multiple explanatory variables missing? 
  - impute one at a time; start w the one with the least amount of missing data 
      - then continue with those and use the 'new' ones as predictors 
      
  - collinearity problems? yes! 
     - PCA it: makes them all independent 
     
Methods of Imputation 
  - step_impute_mean() - quantitative only
  - step_impute_median() - quantitative only 
  - step_impute_mode() - categorical only 
  - step_impute_knn(var, impute_with = , imp_vars(vars to use to impute), neighbors=) - categorical only 
  - bagged trees: step_impute_bag(var, impute_with = , trees= )
  - linreg: step_impute_linear(var, impute_with = ) - quantitative only  
```{r}

gggTest <- read_csv('test.csv')
gggTrain <- read_csv('train.csv')
gggTrainNA <- read_csv('trainWithMissingValues.csv') 
gggTrain
```


```{r}
ggg_recipe <- recipe(type~., data = gggTrain) %>% 
  step_impute_mean(all_numeric_predictors())

prep <- prep(ggg_recipe) 
baked <- bake(prep, new_data = gggTrainNA)  

rmse_vec(gggTrain[is.na(gggTrainNA)], baked[is.na(gggTrainNA)])
```


### 6 November - Neural Networks 

```{r}
library(tidyverse)
library(tidymodels)
library(vroom)
library(embed)
library(themis)

nn_recipe <- recipe(type ~ ., gggTrain) %>%
  update_role(id, new_role="id") %>%
  step_mutate(type = as.factor(type), skip = TRUE) %>%
  step_mutate(color = as.factor(color)) %>%
  step_dummy(color) %>% # Turn color to factor then dummy encode color
  step_range(all_numeric_predictors(), min=0, max=1) 

# Neural Network Model
nn_model <- mlp(hidden_units = tune(),
                epochs = 50) %>% #or 100 or 250
  set_engine("nnet") %>%
  set_mode("classification")

# set workflow
nn_wf <- workflow() %>%
  add_recipe(nn_recipe) %>%
  add_model(nn_model)
nn_tuneGrid <- grid_regular(hidden_units(range=c(1, 75)),
                            levels=3)

# Set up k-fold cross validation and run it
nn_folds <- vfold_cv(gggTrain, v = 5, repeats = 1)
CV_nn_results <- nn_wf %>%
  tune_grid(resamples = nn_folds,
            grid = nn_tuneGrid,
            metrics = metric_set(accuracy))
CV_nn_results %>% collect_metrics() %>% filter(.metric=="accuracy") %>%
ggplot(aes(x=hidden_units, y=mean)) + geom_line()

# Find Best Tuning Parameters
bestTune_nn <- CV_nn_results %>%
  select_best("accuracy")

#finalize workflow and fit it
final_nn_wf <- nn_wf %>%
  finalize_workflow(bestTune_nn) %>%
  fit(gggTrain)
pred_nn <- predict(final_nn_wf, new_data = gggTest, type = "class") %>%
  bind_cols(., gggTest) %>%
  rename(type = .pred_class) %>%
  select(id, type)
vroom_write(pred_nn, "GGG_preds_nn.csv", delim = ",")
```

